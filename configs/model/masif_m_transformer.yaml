_target_: masif.models.masif_transformer.masifMLPTransformer

dataset_metaf_encoder:
  _target_: masif.utils.mlp.MLP

  #  [ ${ dynamically_computed.n_data_meta_features }, 200, 100 ]
  activation: 'relu'
  hidden_dims:
    - ${ dynamically_computed.n_data_meta_features }
    - 128
    - 64

algo_metaf_encoder:
  _target_: masif.utils.mlp.MLP

  activation: 'relu'
  hidden_dims:
    - ${ dynamically_computed.n_algo_meta_features }
    - 128
    - 64
n_algos: ${dynamically_computed.n_algos}
n_fidelities: ${ dynamically_computed.n_fidelities }


transformer_lc:
  _target_: masif.models.masif_transformer_guided_attention.masifGuidedAttentionTransformerEncoder
  num_layers: 2

  metaf_embed_dim: 64 # same as the last element of dataset_metaf_encoder.hidden_dims
  l_seq:
    - ${dynamically_computed.n_fidelities}
    - 1

  hidden_dims: # optional argument, to specify intermediate layers in the guided attention mlp acting on the attention
    - 128

  encoder_layer:

    _target_: masif.models.masif_transformer_guided_attention.GuidedAttentionTransformerEncoderLayer
    _partial_: True
    d_model: 64
    nhead: 4
    dim_feedforward: 128
    dropout: 0.2
    activation: 'relu'
    batch_first: True
    norm_first: True

decoder:
  _target_: masif.utils.mlp.MLP

  #  [ ${ add: ${ model.encoder.hidden_dims[ 0 ] }, ${ transformer_dims[ 0 ] } }, 200, 100 ] # fixme: change index
  activation: 'relu'

  hidden_dims:
    # - ${add:dynamically_computed.n_algos,dynamically_computed.n_algos}
    - 64               # FIXME: dynamically compute based on the
    - 512
    - 512
    # concat dim before the tensor comes to the decoder
    - ${dynamically_computed.n_algos}
